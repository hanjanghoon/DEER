#!/usr/bin/env python3
"""
make_report_score_v5.py  ğŸŸ¢ 2025-09 with Performance Monitoring
â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
- ê° ìƒ˜í”Œë§ˆë‹¤ ì—¬ëŸ¬ í‰ê°€ í”„ë¡¬í”„íŠ¸ í˜¸ì¶œ:
  - request_fulfillment
  - analytical_soundness
  - structural_coherence
  - format_style
  - ethics_information
- ìƒ˜í”ŒÃ—í”„ë¡¬í”„íŠ¸ë¥¼ ì „ë¶€ asyncë¡œ ë™ì‹œì— ì‹¤í–‰ â†’ ì™„ë£Œ í›„ ìƒ˜í”Œë³„ ë³‘í•©
- ì €ì¥ ìœ„ì¹˜ : {output_dir}/{prefix}/others/
      â”œâ”€ 0001.json â€¦ ìƒ˜í”Œë³„ ê²°ê³¼(ë³‘í•© + í‰ê·  í¬í•¨)
      â””â”€ (summary.json ìƒì„± ì•ˆ í•¨)

ìš”êµ¬ì‚¬í•­:
- í”„ë¡¬í”„íŠ¸/ë©”ì‹œì§€ "ê·¸ëŒ€ë¡œ" ì „ì†¡(ë³´ì • ì—†ìŒ)
  - prompt_fnì´ (SYSTEM_PROMPT, USER_PROMPT) íŠœí”Œì„ ë°˜í™˜í•œë‹¤ê³  ê°€ì •
  - _llm_callì€ messagesë¥¼ ê·¸ëŒ€ë¡œ litellmì— ì „ë‹¬(ì‹œìŠ¤í…œ í”„ë¡¬í”„íŠ¸ ë§ë¶™ì´ì§€ ì•ŠìŒ)
- _llm_call: ë‹¨ìˆœ í˜¸ì¶œ(ì˜ˆì™¸ ìƒìœ„ ì „íŒŒ)
- _eval_task: ì–´ë–¤ ì˜ˆì™¸ë“  MAX_RETRY ì¬ì‹œë„(+ë°±ì˜¤í”„), ì´ˆê³¼ ì‹œ ì˜ˆì™¸
- ê°•ì œ N/A ì£¼ì…/í…œí”Œë¦¿ ì œê±°(í˜•ì‹ ë¶ˆì¼ì¹˜ ì‹œ ì‹¤íŒ¨)
- âš  summary.jsonë§Œ ì œê±° â€” ê°œë³„ íŒŒì¼ì˜ ì¢…í•©(í‰ê· ) ê²°ê³¼ëŠ” ìœ ì§€
- â±ï¸  ê° ë‹¨ê³„ë³„ ì„±ëŠ¥ ì¸¡ì • ì¶”ê°€
"""

from __future__ import annotations

import argparse
import asyncio
import json
import os
import random
import re
import sys
import time
from collections import defaultdict
from pathlib import Path
from typing import Any, Dict, List, Union, Tuple

# â”€â”€ í”„ë¡¬í”„íŠ¸ ë¹Œë” import â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
from prompts.prompt_report import (
    get_prompt_request_fulfillment,
    get_prompt_analytical_soundness,
    get_prompt_structural_coherence,
    get_prompt_format_style,
    get_prompt_information_ethics,
)

from prompts.schemas import RESPONSE_FORMAT_MAP

try:
    import litellm
except ImportError:
    sys.exit("litellm not installed. Run: pip install litellm")

MAX_RETRY = 5
BACKOFF = [1, 2, 4, 8, 16]

TOP6 = [
    "request_fulfillment",
    "analytical_soundness",
    "structural_coherence",
    "format_style",
    "information_integrity",
    "ethics_compliance",
]

# â”€â”€ í”„ë¡¬í”„íŠ¸ë³„ ìµœì†Œ ìš”êµ¬ êµ¬ì¡°(ë¶€ë¶„ ê²€ì¦ìš©) â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
REQUEST_FULFILLMENT_STRUCTURE = {"request_fulfillment": ["completeness", "scope", "helpfulness"]}
ANALYTICAL_SOUNDNESS_STRUCTURE = {"analytical_soundness": ["quantification", "reasoning"]}
STRUCTURAL_COHERENCE_STRUCTURE = {"structural_coherence": ["introduction", "body", "conclusion", "section"]}
FORMAT_STYLE_STRUCTURE = {"format_style": ["report_format", "writing_quality", "paragraph_quality", "readability"]}
ETHICS_INFORMATION_STRUCTURE = {
    "information_integrity": ["recency"],
    "ethics_compliance": ["sensitive_handling", "safety_impact", "perspective_balance"],
}

# â”€â”€ ì „ì²´ í•„ìˆ˜ êµ¬ì¡° (ë³‘í•© í›„ ìµœì¢… ê²€ì¦ìš©) â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
REQUIRED: Dict[str, List[str]] = {
    "request_fulfillment": ["completeness", "scope", "helpfulness"],
    "analytical_soundness": ["quantification", "reasoning"],
    "structural_coherence": ["introduction", "body", "conclusion", "section"],
    "format_style": ["report_format", "writing_quality", "paragraph_quality", "readability"],
    "information_integrity": ["recency"],
    "ethics_compliance": ["sensitive_handling", "safety_impact", "perspective_balance"],
}

# â”€â”€ ì–´ë–¤ í”„ë¡¬í”„íŠ¸ë¥¼ ëŒë¦´ì§€ ì •ì˜ â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
PROMPT_SPECS = [
    {"kind": "request_fulfillment", "fn": get_prompt_request_fulfillment, "structure": REQUEST_FULFILLMENT_STRUCTURE, "needs_core": True},
    {"kind": "analytical_soundness", "fn": get_prompt_analytical_soundness, "structure": ANALYTICAL_SOUNDNESS_STRUCTURE, "needs_core": True},
    {"kind": "structural_coherence", "fn": get_prompt_structural_coherence, "structure": STRUCTURAL_COHERENCE_STRUCTURE, "needs_core": True},
    {"kind": "format_style", "fn": get_prompt_format_style, "structure": FORMAT_STYLE_STRUCTURE, "needs_core": False},
    {"kind": "information_ethics", "fn": get_prompt_information_ethics, "structure": ETHICS_INFORMATION_STRUCTURE, "needs_core": False},
]

Num = Union[int, float]


# â”€â”€ ìƒ˜í”Œ ë¦¬ìŠ¤íŠ¸ íŒŒì„œ â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
def _parse_samples(arg: str) -> List[int]:
    if arg is None:
        return []
    arg = arg.strip()
    if not arg:
        return []
    toks = re.split(r"[,\s]+", arg)
    ids: List[int] = []
    for t in toks:
        if not t:
            continue
        if "-" in t:
            a, b = t.split("-", 1)
            a_i, b_i = int(a), int(b)
            if a_i > b_i:
                raise ValueError(f"invalid range '{t}' (start>end)")
            ids.extend(range(a_i, b_i + 1))
        else:
            ids.append(int(t))
    return sorted(set(ids))


# â”€â”€ ê²€ì¦ â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
def _element_block_is_valid(element_data: dict) -> bool:
    if not element_data:
        return True
    keys = set(element_data.keys())
    if keys == {"..."}:
        return True
    return any(k.startswith(("C", "Q")) for k in keys)


def _validate_full(scores: dict) -> bool:
    try:
        for rub, clist in REQUIRED.items():
            if rub not in scores:
                return False
            crit_map = scores[rub]
            if not isinstance(crit_map, dict):
                return False
            for c in clist:
                if c not in crit_map:
                    return False
                elements = crit_map[c]
                if not isinstance(elements, dict):
                    return False
                for _, element_data in elements.items():
                    if not isinstance(element_data, dict):
                        return False
                    if not _element_block_is_valid(element_data):
                        return False
        return True
    except Exception:
        return False


def _safe_num(item) -> Union[int, float, None]:
    """ScoreFactor ê°ì²´ ë˜ëŠ” dictì—ì„œ ìˆ«ì ì¶”ì¶œ"""
    if hasattr(item, "score"):
        score = item.score
        return score if isinstance(score, (int, float)) else None

    if isinstance(item, dict) and "score" in item:
        score = item["score"]
        return score if isinstance(score, (int, float)) else None

    # ë ˆê±°ì‹œ: [desc, score] í˜•íƒœ(í˜¹ì‹œ ë‚¨ì•„ìˆì„ ìˆ˜ ìˆìŒ)
    if isinstance(item, (list, tuple)) and len(item) >= 2:
        last = item[-1]
        return last if isinstance(last, (int, float)) else None

    if isinstance(item, (int, float)):
        return item

    return None


def _merge_scores(base: dict, new: dict) -> dict:
    merged = {rub: dict(base.get(rub, {})) for rub in TOP6}
    for rub, criteria in new.items():
        if rub not in merged:
            merged[rub] = {}
        for crit, elements in (criteria or {}).items():
            if crit not in merged[rub]:
                merged[rub][crit] = {}
            for elem_key, elem_data in (elements or {}).items():
                if elem_key not in merged[rub][crit]:
                    merged[rub][crit][elem_key] = elem_data
    return merged


# â”€â”€ í‰ê·  ê³„ì‚° ë¡œì§ â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
def _calculate_averages(scores: dict) -> Dict[str, Any]:
    """
    ìš”ì†Œë³„ C/Q í‰ê·  â†’ ì—˜ë¦¬ë¨¼íŠ¸ í‰ê·  â†’ í¬ë¦¬í…Œë¦¬ì˜¨ í‰ê·  â†’ ë£¨ë¸Œë¦­ í‰ê·  â†’ overall
    """
    element_avgs, crit_avg, rub_avg = {}, {}, {}
    for rub in TOP6:
        if rub not in scores:
            continue
        element_avgs[rub], crit_avg[rub] = {}, {}
        for crit, elements in scores[rub].items():
            element_avgs[rub][crit] = {}
            element_scores = []
            for element_key, element_data in elements.items():
                c_scores, q_scores = [], []
                for factor_key, factor_value in element_data.items():
                    score = _safe_num(factor_value)
                    if score is None:
                        continue
                    if factor_key.startswith("C"):
                        c_scores.append(score)
                    elif factor_key.startswith("Q"):
                        q_scores.append(score)

                c_avg = round(sum(c_scores) / len(c_scores), 2) if c_scores else "N/A"
                q_avg = round(sum(q_scores) / len(q_scores), 2) if q_scores else "N/A"

                if isinstance(c_avg, (int, float)) and isinstance(q_avg, (int, float)):
                    element_avg = round((c_avg + q_avg) / 2, 2)
                elif isinstance(c_avg, (int, float)):
                    element_avg = c_avg
                elif isinstance(q_avg, (int, float)):
                    element_avg = q_avg
                else:
                    element_avg = "N/A"

                element_avgs[rub][crit][element_key] = {"c_avg": c_avg, "q_avg": q_avg, "element_avg": element_avg}
                if isinstance(element_avg, (int, float)):
                    element_scores.append(element_avg)

            crit_avg[rub][crit] = round(sum(element_scores) / len(element_scores), 2) if element_scores else "N/A"

        rv = [v for v in crit_avg[rub].values() if isinstance(v, (int, float))]
        rub_avg[rub] = round(sum(rv) / len(rv), 2) if rv else "N/A"

    overall_vals = [v for v in rub_avg.values() if isinstance(v, (int, float))]
    overall = round(sum(overall_vals) / len(overall_vals), 2) if overall_vals else "N/A"
    return {"ok": True, "element_avgs": element_avgs, "crit_avg": crit_avg, "rub_avg": rub_avg, "overall": overall}


# â”€â”€ LiteLLM í˜¸ì¶œ(ë©”ì‹œì§€ ê·¸ëŒ€ë¡œ ì „ì†¡) â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
async def _llm_call(msgs: List[Dict[str, str]], model: str, kind: str):
    """
    - messagesë¥¼ "ê·¸ëŒ€ë¡œ" ì „ì†¡(ë³´ì • ì—†ìŒ)
    - response_formatì´ ìˆìœ¼ë©´ Pydantic ê²€ì¦
    - ì˜ˆì™¸ëŠ” ìƒìœ„ ì „íŒŒ
    """
    timings: Dict[str, float] = {}
    t_total_start = time.time()

    t0 = time.time()
    kwargs: Dict[str, Any] = {"model": model, "messages": msgs}

    response_format = RESPONSE_FORMAT_MAP.get(kind)
    if response_format:
        kwargs["response_format"] = response_format

    t1 = time.time()
    timings["kwargs_setup"] = t1 - t0

    response = await litellm.acompletion(**kwargs)

    t2 = time.time()
    timings["llm_api_call"] = t2 - t1

    content = response.choices[0].message.content

    t3 = time.time()
    timings["extract_content"] = t3 - t2

    if response_format:
        m = re.search(r"```json\s*(\{.*?\})\s*```", content, re.DOTALL)
        json_str = m.group(1) if m else content.strip()

        t4 = time.time()
        timings["json_regex"] = t4 - t3

        parsed = response_format.model_validate_json(json_str)

        t5 = time.time()
        timings["pydantic_validate"] = t5 - t4
        timings["total"] = t5 - t_total_start

        print(
            f"    [LLM_CALL] kwargs={timings['kwargs_setup']:.3f}s | "
            f"API={timings['llm_api_call']:.2f}s | "
            f"extract={timings['extract_content']:.4f}s | "
            f"regex={timings['json_regex']:.4f}s | "
            f"pydantic={timings['pydantic_validate']:.3f}s | "
            f"TOTAL={timings['total']:.2f}s"
        )

        return parsed

    t_end = time.time()
    timings["total"] = t_end - t_total_start
    print(
        f"    [LLM_CALL] kwargs={timings['kwargs_setup']:.3f}s | "
        f"API={timings['llm_api_call']:.2f}s | "
        f"TOTAL={timings['total']:.2f}s"
    )
    return content.strip()


async def _eval_task(
    model: str,
    sid: int,
    kind: str,
    prompt_fn,
    query: str,
    report: str,
    core_criteria: str,
    needs_core: bool,
):
    """
    - prompt_fnì€ (SYSTEM_PROMPT, USER_PROMPT) íŠœí”Œì„ ë°˜í™˜í•œë‹¤ê³  ê°€ì •
    - ì–´ë–¤ ì˜ˆì™¸ë“  MAX_RETRY ì¬ì‹œë„(+ë°±ì˜¤í”„), ì´ˆê³¼ ì‹œ ì˜ˆì™¸
    """
    timings: Dict[str, float] = {}
    t_task_start = time.time()

    print(f"[{time.strftime('%H:%M:%S')}] [Sample {sid}] {kind} START")

    # 1) í”„ë¡¬í”„íŠ¸ ìƒì„±
    t0 = time.time()
    system_prompt, user_prompt = (
        prompt_fn(query, report, core_criteria) if needs_core else prompt_fn(query, report)
    )
    t1 = time.time()
    timings["prompt_generation"] = t1 - t0
    print(f"    [PROMPT_GEN] {timings['prompt_generation']:.3f}s")

    msgs = [
        {"role": "system", "content": system_prompt},
        {"role": "user", "content": user_prompt},
    ]

    for attempt in range(1, MAX_RETRY + 1):
        try:
            # 2) LLM í˜¸ì¶œ
            t2 = time.time()
            parsed_obj = await _llm_call(msgs, model, kind)
            t3 = time.time()
            timings["llm_call_total"] = t3 - t2

            # 3) Pydantic â†’ dict
            result_dict = parsed_obj.model_dump(by_alias=True)
            t4 = time.time()
            timings["model_dump"] = t4 - t3
            print(f"    [MODEL_DUMP] {timings['model_dump']:.3f}s")

            # 4) ìµœì†Œ ê²€ì¦
            if "scores" not in result_dict:
                raise ValueError("parse fail: missing 'scores' root key")

            scores = result_dict["scores"]

            t5 = time.time()
            timings["validation"] = t5 - t4
            timings["total_task"] = t5 - t_task_start

            print(
                f"[{time.strftime('%H:%M:%S')}] [Sample {sid}] âœ“ {kind} SUCCESS "
                f"(attempt={attempt}) "
                f"[prompt={timings['prompt_generation']:.2f}s | "
                f"llm={timings['llm_call_total']:.2f}s | "
                f"dump={timings['model_dump']:.3f}s | "
                f"TOTAL={timings['total_task']:.2f}s]"
            )

            return {"sample_id": sid, "kind": kind, "ok": True, "scores": scores}

        except Exception as e:
            print(f"[Sample {sid}] âœ— {kind} error on attempt {attempt}: {e}")
            if attempt >= MAX_RETRY:
                raise RuntimeError(f"[Sample {sid}] {kind} failed after {MAX_RETRY} attempts: {e}") from e

            delay = BACKOFF[min(attempt - 1, len(BACKOFF) - 1)] + random.random()
            print(f"[Sample {sid}] â†» retry in {delay:.2f}s")
            await asyncio.sleep(delay)

    raise RuntimeError(f"[Sample {sid}] {kind} unexpected exit from retry loop")


# â”€â”€ main --------------------------------------------------------------------
async def main():
    ap = argparse.ArgumentParser()
    ap.add_argument("--prefix", default="gpt5_deep")
    ap.add_argument("--eval_model", default="gpt-5.2")
    ap.add_argument("--samples", type=str, default="1-3", help="ì˜ˆ: '1,2,3' ë˜ëŠ” '1-5,8'")
    ap.add_argument("--root", default="data/micro1_csai")
    ap.add_argument("--output_dir", default="output_test")
    ap.add_argument("--env", default=".env")
    ap.add_argument("--max_concurrency", type=int, default=0)
    args = ap.parse_args()

    # ìƒ˜í”Œ ë¦¬ìŠ¤íŠ¸ íŒŒì‹±
    try:
        sample_ids = _parse_samples(args.samples)
        if not sample_ids:
            sys.exit("no samples parsed from --samples")
    except Exception as e:
        sys.exit(f"invalid --samples: {e}")

    from dotenv import load_dotenv

    load_dotenv(args.env)

    # LiteLLM API í‚¤ ì„¤ì •
    os.environ["OPENAI_API_KEY"] = os.getenv("OPENAI_API_KEY", "")
    os.environ["OPENROUTER_API_KEY"] = os.getenv("OPENROUTER_API_KEY", "")

    t_start = time.time()

    root = Path(args.root)
    out_dir = Path(args.output_dir) / args.prefix / "others"
    out_dir.mkdir(parents=True, exist_ok=True)

    # ì‘ì—… ìƒì„±
    tasks: List[asyncio.Task] = []
    qmap: Dict[int, str] = {}
    reports: Dict[int, str] = {}
    cores: Dict[int, str] = {}
    selected_samples: List[int] = []

    for i in sample_ids:
        f = root / str(i)
        qf, rf, cf = f / "query.md", f / f"{args.prefix}_{i}.md", f / "core_criteria.md"
        if not (qf.exists() and rf.exists() and cf.exists()):
            print(f"âš  Skip sample {i}: missing files in {f}")
            continue

        qmap[i] = qf.read_text(encoding="utf-8")
        reports[i] = rf.read_text(encoding="utf-8")
        cores[i] = cf.read_text(encoding="utf-8")
        selected_samples.append(i)

        for spec in PROMPT_SPECS:
            tasks.append(
                asyncio.create_task(
                    _eval_task(
                        args.eval_model,
                        i,
                        spec["kind"],
                        spec["fn"],
                        qmap[i],
                        reports[i],
                        cores[i],
                        spec["needs_core"],
                    )
                )
            )

    if not tasks:
        sys.exit("no runnable tasks (check --samples and file existence)")

    # ë™ì‹œì„± ì œí•œ
    if args.max_concurrency and args.max_concurrency > 0:
        sem = asyncio.Semaphore(args.max_concurrency)

        async def _wrap(coro):
            async with sem:
                return await coro

        tasks = [asyncio.create_task(_wrap(t)) for t in tasks]

    # ì‹¤í–‰
    print(f"\n{'='*60}\nğŸ“Š EVALUATION START")
    print(f"Total tasks: {len(tasks)}")
    print(f"Max concurrency: {args.max_concurrency if args.max_concurrency else 'unlimited'}")
    print(f"{'='*60}\n")

    t_eval_start = time.time()
    results = await asyncio.gather(*tasks)
    t_eval_end = time.time()

    print(f"\n{'='*60}\nâ±ï¸  EVALUATION COMPLETED in {t_eval_end - t_eval_start:.2f}s")
    print(f"{'='*60}\nğŸ’¾ SAVING PER-SAMPLE RESULTS\n{'='*60}")

    # ìƒ˜í”Œë³„ ê²°ê³¼ ë¬¶ê¸°
    by_sample: Dict[int, Dict[str, Any]] = defaultdict(dict)
    for r in results:
        sid = r["sample_id"]
        by_sample[sid][r["kind"]] = r

    for sid in sorted(by_sample.keys()):
        save_timings: Dict[str, float] = {}
        t_save_start = time.time()

        # 1) ë³‘í•©
        parts = by_sample[sid]
        merged_scores = {rub: {} for rub in TOP6}
        for _, r in parts.items():
            merged_scores = _merge_scores(merged_scores, r["scores"])
        t1 = time.time()
        save_timings["merge"] = t1 - t_save_start

        # 2) ê²€ì¦
        if not _validate_full(merged_scores):
            raise RuntimeError(f"final validate fail for sample {sid}: REQUIRED structure incomplete")
        t2 = time.time()
        save_timings["validate"] = t2 - t1

        # 3) í‰ê·  ê³„ì‚°
        avg = _calculate_averages(merged_scores)
        t3 = time.time()
        save_timings["calculate_avg"] = t3 - t2

        # 4) dict ìƒì„±
        out = {
            "sample_id": sid,
            "scores": merged_scores,
            "element_avgs": avg["element_avgs"],
            "criteria_avgs": avg["crit_avg"],
            "score_avgs": avg["rub_avg"],
            "score": avg["overall"],
        }
        t4 = time.time()
        save_timings["dict_create"] = t4 - t3

        # 5) JSON ì§ë ¬í™” + íŒŒì¼ ì“°ê¸°
        json_str = json.dumps(out, ensure_ascii=False, indent=2)
        t5 = time.time()
        save_timings["json_dumps"] = t5 - t4

        (out_dir / f"{sid:04d}.json").write_text(json_str, "utf-8")
        t6 = time.time()
        save_timings["file_write"] = t6 - t5
        save_timings["total"] = t6 - t_save_start

        print(
            f"   âœ“ Sample {sid:04d} (score={out['score']}) "
            f"[merge={save_timings['merge']:.3f}s | "
            f"valid={save_timings['validate']:.4f}s | "
            f"avg={save_timings['calculate_avg']:.3f}s | "
            f"dict={save_timings['dict_create']:.4f}s | "
            f"json={save_timings['json_dumps']:.3f}s | "
            f"write={save_timings['file_write']:.4f}s | "
            f"TOTAL={save_timings['total']:.3f}s]"
        )

    elapsed = time.time() - t_start
    mins, secs = divmod(int(elapsed), 60)
    hours, mins = divmod(mins, 60)
    dur_text = f"{hours}h {mins}m {secs}s" if hours else f"{mins}m {secs}s"

    print(f"\n{'='*60}\nâœ… COMPLETE")
    print(f"   Samples requested : {len(selected_samples)}")
    print(f"   Samples processed : {len(by_sample)}")
    print(f"   Evaluation time   : {t_eval_end - t_eval_start:.2f}s")
    print(f"   Total duration    : {dur_text}")
    print(f"   Output directory  : {out_dir}\n{'='*60}\n")

    # ë‚¨ì€ íƒœìŠ¤í¬ ì •ë¦¬
    pending = [t for t in asyncio.all_tasks() if t is not asyncio.current_task()]
    if pending:
        print(f"ğŸ§¹ Cleaning up {len(pending)} background tasks...")
        for task in pending:
            task.cancel()
        await asyncio.gather(*pending, return_exceptions=True)

    print("âœ… Cleanup complete")


if __name__ == "__main__":
    asyncio.run(main())
    print("ğŸ”š Script finished")
