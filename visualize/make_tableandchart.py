#!/usr/bin/env python3
"""
make_tableandchart_domain.py
â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
â€¢ output/<domain>/<model>/final/<task_id>.json â†’ ë„ë©”ì¸ë³„ í‰ê·  ì ìˆ˜ ê³„ì‚° â†’ ì‹œê°í™”(PNG)Â·Excel
â€¢ ê° ë„ë©”ì¸ë³„ë¡œ ëª¨ë“  task íŒŒì¼ë“¤ì˜ í‰ê· ì„ ê³„ì‚°í•˜ì—¬ ëª¨ë¸ ë¹„êµ
â€¢ ì „ì²´ í†µí•© ê²°ê³¼ + ë„ë©”ì¸ë³„ ê²°ê³¼ ìƒì„±
â€¢ CSAI ë„ë©”ì¸ì„ AI(1-5)ì™€ CS(6-10)ë¡œ ë¶„ë¦¬ ì²˜ë¦¬
â€¢ ë ˆì´ì•„ì›ƒ
    â‘  Radar  (1 í–‰ ì „ì²´í­)
    â‘¡ Overall-bar (1 í–‰ ì „ì²´í­)
    â‘¢ Rubric-bar (ì•„ë˜ 2 ì—´ì”©)

ì‚¬ìš©ë²•:
    ì•„ë˜ ì„¤ì • ì„¹ì…˜ì„ ìˆ˜ì •í•œ í›„ ì‹¤í–‰: python make_tableandchart_domain.py
"""
from __future__ import annotations
import json, math, sys
from math import ceil
from pathlib import Path
from typing import Dict, List

import matplotlib.pyplot as plt
import matplotlib.gridspec as gs
import numpy as np
import pandas as pd

# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
# ğŸ“ ì„¤ì • ì„¹ì…˜ - ì—¬ê¸°ë¥¼ ìˆ˜ì •í•˜ì„¸ìš”!
# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

# â”€â”€ ë°ì´í„° ê²½ë¡œ ì„¤ì • â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
ROOT_DIR = "output"          # ë„ë©”ì¸ í´ë”ë“¤ì´ ìˆëŠ” ë£¨íŠ¸ ë””ë ‰í† ë¦¬
OUTPUT_DIR = "visualize"     # ê²°ê³¼ë¥¼ ì €ì¥í•  ë””ë ‰í† ë¦¬

# â”€â”€ ë„ë©”ì¸ ì„¤ì • â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
# ë¶„ì„í•  ë„ë©”ì¸ ëª©ë¡ (ì›í•˜ì§€ ì•ŠëŠ” ë„ë©”ì¸ì€ ì£¼ì„ ì²˜ë¦¬)
# ì£¼ì˜: "ai"ì™€ "cs"ëŠ” ì‹¤ì œë¡œëŠ” "csai" í´ë”ë¥¼ ì°¸ì¡°í•©ë‹ˆë‹¤
DOMAINS = [
    "ai",            # â† CSAIì˜ 1-5ë²ˆ task (AI)
    "bio",           # â† bio ë„ë©”ì¸ ì²˜ë¦¬
    "chem",          # â† chem ë„ë©”ì¸ ì²˜ë¦¬
    "cs",            # â† CSAIì˜ 6-10ë²ˆ task (CS)
    "economics",
    "education",
    "engineer",
    "history",
    "linguistics",
    "math",
    "philosophy",
    "physics",
    "psychology"
]

# ì˜ˆì‹œ: ì¼ë¶€ë§Œ ì²˜ë¦¬í•˜ë ¤ë©´
# DOMAINS = [
#     "bio",
#     # "chem",      # â† ì£¼ì„ ì²˜ë¦¬í•˜ë©´ ì œì™¸
#     "ai",
# ]

# â”€â”€ ëª¨ë¸ ê·¸ë£¹ ì •ì˜ ë° ì„ íƒ â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
# ì›í•˜ì§€ ì•ŠëŠ” ê·¸ë£¹ ì „ì²´ë¥¼ ì£¼ì„ ì²˜ë¦¬í•˜ì„¸ìš”
MODEL_GROUPS = {
    "fast": [        # â† fast ê·¸ë£¹ (ì´ ì¤„ë¶€í„° ], ê¹Œì§€ ì£¼ì„ ì²˜ë¦¬í•˜ë©´ ì œì™¸)
        # "qwen3-235b-fast",
        # "gemini-2.5-pro-fast",
        # "claude_opus4.1_fast",
        "gpt5.2_fast",
    ],
    # "think": [       # â† think ê·¸ë£¹
    #     "qwen3-235b-think",
    #     "gemini-2.5-pro-think",
    #     "claude_opus4.1_think",
    #     "gpt5.2_think",
    # ],
    # "think_search": [ # â† think_search ê·¸ë£¹
    #     "qwen3-235b-think_search",
    #     "claude_opus4.1_think_search",
    #     "gpt5.2_think_search",
    # ],
    # "deep": [        # â† deep ê·¸ë£¹
    #     "webthinker",
    #     "qwen3-235b-deep",
    #     "gemini-2.5-pro_deep",
    #     "claude_opus4.1_deep",
    #     "gpt5_deep",
    #     # "chatexaone_251208_deepresearch"
    # ],
}

# â”€â”€ ì‹œê°í™” ì„¤ì • â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
DPI = 150          # ì´ë¯¸ì§€ í•´ìƒë„ (150~300 ê¶Œì¥)
SHOW_PLOT = False  # Trueë©´ í™”ë©´ì— í‘œì‹œ

# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
# âš™ï¸ ë‚´ë¶€ ì„¤ì • - ìˆ˜ì •í•˜ì§€ ë§ˆì„¸ìš”
# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

# ëª¨ë“  ëª¨ë¸ ëª©ë¡ ìƒì„± (MODEL_GROUPSì—ì„œ ìë™ ìˆ˜ì§‘)
ALL_MODELS = []
for group_name, models in MODEL_GROUPS.items():
    ALL_MODELS.extend(models)

# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
# ì¶•ì•½ ë ˆì´ë¸”
# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
SHORT = {
    # ìƒˆë¡œìš´ êµ¬ì¡°
    "request_fulfillment":    "fulfillment",
    "analytical_soundness":   "analytical",
    "structural_coherence":   "structure",
    "format_style":           "format",
    
    # ê¸°ì¡´ ìœ ì§€
    "information_integrity":  "info_integrity",
    "information_sufficiency":"info_sufficiency",
    "ethics_compliance":      "ethics",
}
# â”€â”€ ì„¸ë¶€ ê¸°ì¤€(criteria) ì¶•ì•½ ë ˆì´ë¸” (ê¸°ì¡´ ìœ ì§€ + ìƒˆ í•­ëª© ì¶”ê°€) â”€â”€
CRITERIA_SHORT = {
    # request_fulfillment (ìƒˆ)
    "completeness": "completeness",
    "scope": "scope",
    "helpfulness": "helpfulness",

    # analytical_soundness (ìƒˆ)
    "quantification": "quantification",
    "reasoning": "reasoning",

    # structural_coherence (ìƒˆ)
    "introduction": "intro",
    "body": "body",
    "conclusion": "conclusion",
    "section": "section",

    # format_style (ìƒˆ)
    "report_format": "report_fmt",
    "writing_quality": "writing",
    "paragraph_quality": "paragraph",
    "readability": "readability",

    # information_integrity (ê¸°ì¡´ ìœ ì§€)
    "claim_factuality": "claim_acc",
    "citation_validity": "citation_acc",
    "reference_accuracy": "ref_accuracy",
    "reference_quality": "ref_quality",
    "reference_diversity": "ref_diversity",

    # information_sufficiency (ê¸°ì¡´ ìœ ì§€)
    "source_support": "support",
    "information": "info_amount",
    "citations": "cites_amount",
    "references": "refs_amount",

    # ethics_compliance (ìƒˆ êµ¬ì¡°)
    "sensitive_handling": "sensitive",
    "safety_impact": "safety",
    "perspective_balance": "balance"
}


# SHORT ì‚¬ì „ì— ì •ì˜ëœ í‚¤ ìˆœì„œë¥¼ ë³´ì¡´í•˜ê¸° ìœ„í•œ ìš°ì„ ìˆœìœ„ ëª©ë¡
_ORDER_PREF = list(SHORT.keys())

# â”€â”€ ê° ë£¨ë¸Œë¦­ ë‚´ë¶€ criteria ìˆœì„œ ì •ì˜ â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
CRITERIA_ORDER = {
    # ìƒˆë¡œìš´ êµ¬ì¡°
    "request_fulfillment": ["completeness", "scope", "helpfulness"],
    "analytical_soundness": ["quantification", "reasoning"],
    "structural_coherence": ["introduction", "body", "conclusion", "section"],
    "format_style": ["report_format", "writing_quality", "paragraph_quality", "readability"],
    
    # ê¸°ì¡´ ìœ ì§€
    "information_integrity": ["claim_factuality", "citation_validity", "reference_accuracy", "reference_quality", "reference_diversity"],
    "information_sufficiency": ["source_support", "information", "citations", "references"],
    
    # ìƒˆ êµ¬ì¡° (sensitive_issues ì œê±°)
    "ethics_compliance": ["sensitive_handling", "safety_impact", "perspective_balance"]
}

# â”€â”€ ìˆ«ì ë³€í™˜ í—¬í¼ â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€

def _to_num(v):
    """ìˆ«ìë¡œ ë³€í™˜. ë³€í™˜ ë¶ˆê°€/ê²°ì¸¡ì€ None."""
    if v is None:
        return None
    if isinstance(v, (int, float)):
        return float(v)
    if isinstance(v, str):
        if v.strip().upper() == "N/A":
            return None
        try:
            return float(v)
        except ValueError:
            return None
    return None

# â”€â”€ ë„ë©”ì¸ ë§¤í•‘ í—¬í¼ â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€

def get_physical_domain(domain: str) -> str:
    """ë…¼ë¦¬ì  ë„ë©”ì¸ëª…ì„ ë¬¼ë¦¬ì  í´ë”ëª…ìœ¼ë¡œ ë³€í™˜"""
    if domain in ["ai", "cs"]:
        return "csai"
    return domain

def get_task_filter(domain: str) -> tuple[int, int] | None:
    """ë„ë©”ì¸ì— ë”°ë¥¸ task ë²ˆí˜¸ í•„í„° ë°˜í™˜ (start, end) ë˜ëŠ” None"""
    if domain == "ai":
        return (1, 5)  # 01.json ~ 05.json
    elif domain == "cs":
        return (6, 10)  # 06.json ~ 10.json
    return None

# â”€â”€ ë„ë©”ì¸ë³„ ë°ì´í„° ë¡œë”© â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€

def load_domain_summary(domain_path: Path, model_name: str, task_filter: tuple[int, int] | None = None) -> dict | None:
    """
    domain_path/<model>/final/*.json íŒŒì¼ë“¤ì„ ëª¨ë‘ ì½ì–´ì„œ í‰ê·  ê³„ì‚°
    
    Args:
        domain_path: ë„ë©”ì¸ ê²½ë¡œ
        model_name: ëª¨ë¸ ì´ë¦„
        task_filter: (start, end) task ë²ˆí˜¸ ë²”ìœ„ í•„í„°. Noneì´ë©´ ëª¨ë“  task í¬í•¨
    
    Returns:
        {
            "score_avgs": {...},
            "criteria_avgs": {...},
            "task_count": N  # ì½ì€ task íŒŒì¼ ê°œìˆ˜
        }
    """
    model_dir = domain_path / model_name / "final"
    if not model_dir.is_dir():
        return None
    
    all_scores = []
    all_criteria = {}
    
    # ëª¨ë“  json íŒŒì¼ ì½ê¸°
    json_files = list(model_dir.glob("*.json"))
    if not json_files:
        return None
    
    for json_file in json_files:
        # task ë²ˆí˜¸ í•„í„°ë§
        if task_filter is not None:
            try:
                # íŒŒì¼ëª…ì—ì„œ ë²ˆí˜¸ ì¶”ì¶œ (ì˜ˆ: "01.json" -> 1, "003.json" -> 3)
                task_num = int(json_file.stem.lstrip('0') or '0')
                start, end = task_filter
                if not (start <= task_num <= end):
                    continue  # ë²”ìœ„ ë°–ì´ë©´ ìŠ¤í‚µ
            except ValueError:
                continue  # ìˆ«ìê°€ ì•„ë‹ˆë©´ ìŠ¤í‚µ
        
        try:
            data = json.loads(json_file.read_text(encoding="utf-8"))
            all_scores.append(data.get("score_avgs", {}))
            
            # criteria_avgs ìˆ˜ì§‘
            criteria_data = data.get("criteria_avgs", {})
            for rubric, crit_dict in criteria_data.items():
                if rubric not in all_criteria:
                    all_criteria[rubric] = {}
                for crit_key, val in crit_dict.items():
                    if crit_key not in all_criteria[rubric]:
                        all_criteria[rubric][crit_key] = []
                    all_criteria[rubric][crit_key].append(val)
        except Exception as e:
            print(f"Warning: Failed to load {json_file}: {e}")
            continue
    
    if not all_scores:
        return None
    
    # score_avgs í‰ê·  ê³„ì‚°
    score_avgs = {}
    all_keys = set()
    for s in all_scores:
        all_keys.update(s.keys())
    
    for key in all_keys:
        vals = [s.get(key) for s in all_scores if key in s]
        vals = [_to_num(v) for v in vals]
        vals = [v for v in vals if v is not None and not math.isnan(v)]
        if vals:
            score_avgs[key] = sum(vals) / len(vals)
    
    # criteria_avgs í‰ê·  ê³„ì‚°
    criteria_avgs = {}
    for rubric, crit_dict in all_criteria.items():
        criteria_avgs[rubric] = {}
        for crit_key, vals in crit_dict.items():
            nums = [_to_num(v) for v in vals]
            nums = [v for v in nums if v is not None and not math.isnan(v)]
            if nums:
                criteria_avgs[rubric][crit_key] = sum(nums) / len(nums)
    
    return {
        "score_avgs": score_avgs,
        "criteria_avgs": criteria_avgs,
        "task_count": len(all_scores)
    }

def scan_domain(root: Path, domain: str, models_list: List[str]) -> dict:
    """
    íŠ¹ì • ë„ë©”ì¸ì˜ ì§€ì •ëœ ëª¨ë¸ ë°ì´í„° ìŠ¤ìº”
    AI/CSì˜ ê²½ìš° csai í´ë”ì—ì„œ task ë²ˆí˜¸ë¡œ í•„í„°ë§
    
    Returns:
        {model_name: summary_dict, ...}
    """
    physical_domain = get_physical_domain(domain)
    task_filter = get_task_filter(domain)
    
    domain_path = root / physical_domain
    if not domain_path.is_dir():
        print(f"Warning: Domain directory not found: {domain_path}")
        return {}
    
    out = {}
    for model_name in models_list:
        summary = load_domain_summary(domain_path, model_name, task_filter)
        if summary:
            out[model_name] = summary
            if task_filter:
                print(f"  {domain}/{model_name}: {summary['task_count']} tasks (filtered {task_filter[0]}-{task_filter[1]})")
            else:
                print(f"  {domain}/{model_name}: {summary['task_count']} tasks")
    
    return out

# â”€â”€ ì „ì²´ ë„ë©”ì¸ í‰ê·  ê³„ì‚° â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€

def aggregate_domains(domain_summaries: Dict[str, Dict[str, dict]]) -> Dict[str, dict]:
    """
    ì—¬ëŸ¬ ë„ë©”ì¸ì˜ ê²°ê³¼ë¥¼ ëª¨ë¸ë³„ë¡œ í‰ê· ë‚´ì–´ ì „ì²´ í†µí•© ê²°ê³¼ ìƒì„±
    
    Args:
        domain_summaries: {domain_name: {model_name: summary_dict}}
    
    Returns:
        {model_name: aggregated_summary_dict}
    """
    # ëª¨ë“  ëª¨ë¸ ëª©ë¡ ìˆ˜ì§‘
    all_models = set()
    for domain_data in domain_summaries.values():
        all_models.update(domain_data.keys())
    
    aggregated = {}
    
    for model in all_models:
        # ì´ ëª¨ë¸ì´ í¬í•¨ëœ ëª¨ë“  ë„ë©”ì¸ì˜ ë°ì´í„° ìˆ˜ì§‘
        model_scores = []
        model_criteria = {}
        
        for domain, domain_data in domain_summaries.items():
            if model not in domain_data:
                continue
            
            summary = domain_data[model]
            model_scores.append(summary["score_avgs"])
            
            # criteria_avgs ìˆ˜ì§‘
            for rubric, crit_dict in summary["criteria_avgs"].items():
                if rubric not in model_criteria:
                    model_criteria[rubric] = {}
                for crit_key, val in crit_dict.items():
                    if crit_key not in model_criteria[rubric]:
                        model_criteria[rubric][crit_key] = []
                    model_criteria[rubric][crit_key].append(val)
        
        # score_avgs í‰ê·  ê³„ì‚°
        score_avgs = {}
        all_keys = set()
        for s in model_scores:
            all_keys.update(s.keys())
        
        for key in all_keys:
            vals = [s.get(key) for s in model_scores if key in s]
            vals = [_to_num(v) for v in vals]
            vals = [v for v in vals if v is not None and not math.isnan(v)]
            if vals:
                score_avgs[key] = sum(vals) / len(vals)
        
        # criteria_avgs í‰ê·  ê³„ì‚°
        criteria_avgs = {}
        for rubric, crit_dict in model_criteria.items():
            criteria_avgs[rubric] = {}
            for crit_key, vals in crit_dict.items():
                nums = [_to_num(v) for v in vals]
                nums = [v for v in nums if v is not None and not math.isnan(v)]
                if nums:
                    criteria_avgs[rubric][crit_key] = sum(nums) / len(nums)
        
        aggregated[model] = {
            "score_avgs": score_avgs,
            "criteria_avgs": criteria_avgs,
        }
    
    return aggregated

# â”€â”€ plotting helpers â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€

def setup_radar(ax, labels):
    ang = np.linspace(0, 2 * np.pi, len(labels), endpoint=False).tolist()
    ax.set_theta_offset(np.pi / 2)
    ax.set_theta_direction(-1)
    ax.set_xticks(ang)
    ax.set_xticklabels([SHORT.get(l, l) for l in labels], fontsize=7)
    ax.set_ylim(0, 10)
    ax.set_yticks([2, 6, 10])
    ax.set_yticklabels(["2", "6", "10"], fontsize=5)
    ax.grid(True, lw=.3)
    return ang

def draw_radar(ax, ang, vals, *, color, label):
    nums = [_to_num(v) for v in vals]
    vv = [0 if (v is None or (isinstance(v, float) and math.isnan(v))) else v for v in nums]
    ax.plot(ang + ang[:1], vv + vv[:1], color=color, lw=1.2, marker="o", ms=2, label=label)

def dynamic_ylim(vals):
    vv = [v for v in vals if isinstance(v, (int, float)) and not math.isnan(v)]
    if not vv:
        return 0, 10
    lo, hi = min(vv), max(vv)
    pad = max(.5, (hi - lo) * .15)
    lo = max(0, math.floor((lo - pad) * 2) / 2)
    hi = min(10, math.ceil((hi + pad) * 2) / 2)
    if hi - lo < 2:
        mid = (lo + hi) / 2
        lo, hi = mid - 1, mid + 1
    return lo, hi

def draw_bar(ax, xlab, models, summary, *, rubric=None, colors, title, show_legend=False):
    """ê·¸ë£¹í˜• ë§‰ëŒ€ê·¸ë˜í”„ë¥¼ ê·¸ë¦½ë‹ˆë‹¤."""
    x = np.arange(len(xlab))
    n_models = len(models)
    width = 0.8 / n_models
    allv = []

    for idx, m in enumerate(models):
        if rubric:
            crit_map = summary.get(m, {}).get("criteria_avgs", {}).get(rubric, {})
            y = [crit_map.get(c, math.nan) for c in xlab]
        else:
            y = [summary.get(m, {}).get("score_avgs", {}).get(c, math.nan) for c in xlab]

        nums = [_to_num(v) for v in y]
        y_plot = [0 if (v is None or (isinstance(v, float) and math.isnan(v))) else v for v in nums]
        offsets = x + (idx - (n_models - 1) / 2) * width
        ax.bar(offsets, y_plot, width=width, color=colors[m], label=m, alpha=0.8)
        allv += [v for v in nums if (v is not None and not (isinstance(v, float) and math.isnan(v)))]

        if all((v is None) or (isinstance(v, float) and math.isnan(v)) for v in nums):
            ax.text(0.5, 0.5, "no data", ha="center", va="center",
                    transform=ax.transAxes, fontsize=9, alpha=0.7)

    lo, hi = dynamic_ylim(allv)
    ax.set_ylim(lo, hi)
    ax.set_yticks(np.arange(lo, hi + .001, .5))
    ax.set_yticklabels([f"{t:g}" for t in np.arange(lo, hi + .001, .5)], fontsize=6)
    ax.set_xticks(x)

    if rubric:
        ax.set_xticklabels([CRITERIA_SHORT.get(t, t) for t in xlab], rotation=30, ha="right", fontsize=8)
    else:
        ax.set_xticklabels([SHORT.get(t, t) for t in xlab], rotation=30, ha="right", fontsize=8)

    ax.grid(axis="y", lw=.3, alpha=.5)
    ax.set_title(title, fontsize=10)

    if show_legend and len(models) > 1:
        ax.legend(fontsize=6, loc='upper right')

# â”€â”€ Excel helpers â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€

def _auto(ws):
    from openpyxl.utils import get_column_letter
    for col in ws.columns:
        ws.column_dimensions[get_column_letter(col[0].column)].width = \
            min(40, max(4, max(len(str(c.value)) if c.value else 0 for c in col) + 2))

def export_excel(summary, path: Path, top_keys: List[str], crit: Dict[str, List[str]], sheet_prefix=""):
    """
    Excel ë‚´ë³´ë‚´ê¸°:
      - overview: TOP ë£¨ë¸Œë¦­ë“¤ + MEAN ì—´ (ëŒ€ë¶„ë¥˜)
      - <rubric>: ë£¨ë¸Œë¦­ë³„ ì‹œíŠ¸ + MEAN ì—´ (ì†Œë¶„ë¥˜)
      - criteria_all: ëª¨ë“  ì„¸ë¶€ ê¸°ì¤€ + MEAN_ALL ì—´
      - ê° ê·¸ë£¹ ì‚¬ì´ì— ê·¸ë£¹ëª… í–‰ ì¶”ê°€
      - ì†Œìˆ˜ì  ë‘˜ì§¸ìë¦¬ê¹Œì§€ í‘œì‹œ
    """
    def _label_crit(rub: str, ck: str) -> str:
        left = SHORT.get(rub, rub)
        right = CRITERIA_SHORT.get(ck, ck)
        return f"{left}/{right}"
    
    def _format_value(val):
        """ê°’ì„ ì†Œìˆ˜ì  ë‘˜ì§¸ìë¦¬ë¡œ í¬ë§·íŒ…"""
        if val is None or (isinstance(val, float) and math.isnan(val)):
            return np.nan  # â† np.nan ë°˜í™˜
        return round(val, 2)


    flat_cols = []
    for rub in top_keys:
        cks = crit.get(rub, [])
        for ck in cks:
            flat_cols.append((rub, ck, _label_crit(rub, ck)))

    path.parent.mkdir(parents=True, exist_ok=True)
    
    with pd.ExcelWriter(path, engine="openpyxl") as w:
        # â”€â”€ overview (ëŒ€ë¶„ë¥˜: score_avgs) â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
        rows = []
        for group_name, models_in_group in MODEL_GROUPS.items():
            # ê·¸ë£¹ í—¤ë” í–‰ ì¶”ê°€
            group_row = {"model": group_name}
            for a in top_keys:
                group_row[a] = None
            group_row["MEAN"] = None
            rows.append(group_row)
            
            # ê·¸ë£¹ ë‚´ ëª¨ë¸ë“¤
            for m in models_in_group:
                if m not in summary:
                    continue
                row = {"model": m}
                for a in top_keys:
                    row[a] = _format_value(summary[m]["score_avgs"].get(a, math.nan))
                rows.append(row)
        
        df_over = pd.DataFrame(rows).set_index("model")
        # MEAN ê³„ì‚° (ê·¸ë£¹ í—¤ë” ì œì™¸)
        for idx in df_over.index:
            if idx in MODEL_GROUPS:  # ê·¸ë£¹ í—¤ë”ë©´ ìŠ¤í‚µ
                continue
            df_over.loc[idx, "MEAN"] = _format_value(df_over.loc[idx, top_keys].mean(skipna=True))
        
        sheet_name = f"{sheet_prefix}overview" if sheet_prefix else "overview"
        df_over.to_excel(w, sheet_name)

        # â”€â”€ rubric-wise (ê° ë£¨ë¸Œë¦­ ì‹œíŠ¸ - ì†Œë¶„ë¥˜) â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
        for rub in top_keys:
            rows = []
            cks = crit.get(rub, [])
            
            for group_name, models_in_group in MODEL_GROUPS.items():
                # ê·¸ë£¹ í—¤ë” í–‰ ì¶”ê°€
                group_row = {"model": group_name}
                for ck in cks:
                    group_row[ck] = None
                if cks:
                    group_row["MEAN"] = None
                rows.append(group_row)
                
                # ê·¸ë£¹ ë‚´ ëª¨ë¸ë“¤
                for m in models_in_group:
                    if m not in summary:
                        continue
                    row = {"model": m}
                    src = summary[m]["criteria_avgs"].get(rub, {})
                    for ck in cks:
                        row[ck] = _format_value(src.get(ck, math.nan))
                    rows.append(row)
            
            df_rub = pd.DataFrame(rows).set_index("model")
            # MEAN ê³„ì‚° (ê·¸ë£¹ í—¤ë” ì œì™¸)
            if len(cks) > 0:
                for idx in df_rub.index:
                    if idx in MODEL_GROUPS:  # ê·¸ë£¹ í—¤ë”ë©´ ìŠ¤í‚µ
                        continue
                    df_rub.loc[idx, "MEAN"] = _format_value(df_rub.loc[idx, cks].mean(skipna=True))
            
            rename_map = {ck: CRITERIA_SHORT.get(ck, ck) for ck in cks}
            if "MEAN" in df_rub.columns:
                rename_map["MEAN"] = "MEAN"
            sheet_name = f"{sheet_prefix}{rub}" if sheet_prefix else rub
            df_rub.rename(columns=rename_map).to_excel(w, sheet_name[:31])

        # â”€â”€ criteria_all (ëª¨ë“  ì†Œë¶„ë¥˜) â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
        rows_all = []
        for group_name, models_in_group in MODEL_GROUPS.items():
            # ê·¸ë£¹ í—¤ë” í–‰ ì¶”ê°€
            group_row = {"model": group_name}
            for rub, ck, _disp in flat_cols:
                group_row[_disp] = None
            group_row["MEAN_ALL"] = None
            rows_all.append(group_row)
            
            # ê·¸ë£¹ ë‚´ ëª¨ë¸ë“¤
            for m in models_in_group:
                if m not in summary:
                    continue
                row = {"model": m}
                for rub, ck, _disp in flat_cols:
                    val = summary[m]["criteria_avgs"].get(rub, {}).get(ck, math.nan)
                    row[_disp] = _format_value(val)
                rows_all.append(row)

        df_all = pd.DataFrame(rows_all).set_index("model")
        crit_cols_display = [disp for _, _, disp in flat_cols]
        # MEAN_ALL ê³„ì‚° (ê·¸ë£¹ í—¤ë” ì œì™¸)
        if crit_cols_display:
            for idx in df_all.index:
                if idx in MODEL_GROUPS:  # ê·¸ë£¹ í—¤ë”ë©´ ìŠ¤í‚µ
                    continue
                df_all.loc[idx, "MEAN_ALL"] = _format_value(df_all.loc[idx, crit_cols_display].mean(skipna=True))

        sheet_name = f"{sheet_prefix}criteria_all" if sheet_prefix else "criteria_all"
        df_all.to_excel(w, sheet_name[:31])

        # â”€â”€ ì—´ ë„ˆë¹„ ìë™í™” â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
        for ws in w.book.worksheets:
            _auto(ws)

# â”€â”€ sheet builders â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€

def _base(rows_rub: int, dpi: int):
    height = [2] + [1] + [1] * rows_rub
    fig = plt.figure(figsize=(12, (sum(height)) * 3), dpi=dpi, constrained_layout=True)
    grid = gs.GridSpec(2 + rows_rub, 2, hspace=0.8, wspace=0.4, height_ratios=height)
    return fig, grid

def combined_sheet(models, summary, out, dpi, show, top_keys, crit, title_suffix=""):
    rows_rub = ceil(len(top_keys) / 2)
    fig, grid = _base(rows_rub, dpi)
    cmap = plt.get_cmap("tab10")
    colors = {m: cmap(i % 10) for i, m in enumerate(models)}

    ax_r = fig.add_subplot(grid[0, :], polar=True)
    ang = setup_radar(ax_r, top_keys)
    for m in models:
        draw_radar(ax_r, ang,
                   [summary[m]["score_avgs"].get(a, math.nan) for a in top_keys],
                   color=colors[m], label=m)
    ax_r.set_title("overview radar", fontsize=10)
    ax_r.legend(loc="upper left", bbox_to_anchor=(1.22, 1.02), frameon=False, fontsize=6)

    draw_bar(fig.add_subplot(grid[1, :]), top_keys, models, summary,
             colors=colors, title="overall bar", show_legend=True)

    for i, rub in enumerate(top_keys):
        r = 2 + i // 2
        c = i % 2
        draw_bar(fig.add_subplot(grid[r, c]), crit[rub], models, summary,
                 rubric=rub, colors=colors, title=f"{rub} bar", show_legend=True)

    fig.legend(handles=[plt.Line2D([], [], lw=2, color=colors[m], label=m) for m in models],
               loc="center left", bbox_to_anchor=(1.03, 0.5), frameon=False, fontsize=7)

    fig.suptitle(f"Model comparison{title_suffix}", y=.97, fontsize=14)
    plt.subplots_adjust(top=0.95)
    out.parent.mkdir(parents=True, exist_ok=True)
    fig.savefig(out, dpi=dpi)
    if show:
        plt.show()
    plt.close(fig)

# â”€â”€ ì •ë ¬ í—¬í¼ â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€

def _sort_criteria_keys(rubric: str, raw_keys: List[str]) -> List[str]:
    """ê° ë£¨ë¸Œë¦­ë³„ë¡œ ì •ì˜ëœ criteria ìˆœì„œë¥¼ ì ìš©í•˜ëŠ” í•¨ìˆ˜."""
    if rubric in CRITERIA_ORDER:
        ordered = [k for k in CRITERIA_ORDER[rubric] if k in raw_keys]
        remaining = [k for k in raw_keys if k not in CRITERIA_ORDER[rubric]]
        return ordered + remaining
    else:
        return sorted(raw_keys)

def _sort_top_keys(raw_keys: List[str]) -> List[str]:
    """SHORT ë”•ì…”ë„ˆë¦¬ì˜ í‚¤ ìˆœì„œë¥¼ ìš°ì„ ì ìœ¼ë¡œ ì ìš©í•œ ì •ë ¬ í•¨ìˆ˜."""
    ordered = [k for k in _ORDER_PREF if k in raw_keys]
    remaining = [k for k in raw_keys if k not in _ORDER_PREF]
    return ordered + remaining

# â”€â”€ main â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€

def main():
    root = Path(ROOT_DIR)
    if not root.exists():
        sys.exit(f"âŒ Root directory not found: {root}")

    # ë„ë©”ì¸ ëª©ë¡ (ìƒë‹¨ í•˜ë“œì½”ë”©ì—ì„œ ê°€ì ¸ì˜¤ê¸°)
    domains = DOMAINS
    
    if not domains:
        sys.exit("âŒ No domains specified in DOMAINS. Please check the configuration at the top of the script.")

    # ì²˜ë¦¬í•  ëª¨ë¸ ëª©ë¡ (ìƒë‹¨ í•˜ë“œì½”ë”©ì—ì„œ ìë™ ìƒì„±)
    models = ALL_MODELS
    
    if not models:
        sys.exit("\nâŒ No models specified in MODEL_GROUPS. Please check the configuration at the top of the script.")
    
    print(f"\n{'='*60}")
    print(f"ğŸ“‚ Active domains: {', '.join(domains)}")
    print(f"   Note: 'ai' and 'cs' are split from 'csai' folder")
    print(f"ğŸ“Š Selected models ({len(models)}):")
    print(f"{'='*60}")
    for m in models:
        print(f"  â€¢ {m}")

    # ê° ë„ë©”ì¸ë³„ë¡œ ë°ì´í„° ìˆ˜ì§‘
    domain_summaries = {}
    print(f"\n{'='*60}")
    print("ğŸ“‚ Loading data from domains...")
    print(f"{'='*60}")
    
    for domain in domains:
        print(f"\n[{domain.upper()}]")
        summary = scan_domain(root, domain, models)
        if summary:
            domain_summaries[domain] = summary
        else:
            print(f"  âš ï¸  No data found")

    if not domain_summaries:
        sys.exit("\nâŒ No data found in any domain")

    # ì „ì²´ í†µí•© ê²°ê³¼ ìƒì„± (ëª¨ë“  ë„ë©”ì¸ í‰ê· )
    print(f"\n{'='*60}")
    print("ğŸ”„ Aggregating all domains...")
    print(f"{'='*60}")
    aggregated_summary = aggregate_domains(domain_summaries)
    
    # ì‹¤ì œ ë°ì´í„°ê°€ ìˆëŠ” ëª¨ë¸ë§Œ í•„í„°ë§ (MODEL_GROUPS ìˆœì„œ ìœ ì§€)
    present_models = []
    for group_name, group_models in MODEL_GROUPS.items():
        for m in group_models:
            if m in aggregated_summary:
                present_models.append(m)
    print(f"âœ“ Models with data: {len(present_models)}")

    if not present_models:
        sys.exit("\nâŒ No models with data found")

    # TOP/CRIT í‚¤ ìƒì„±
    raw_top_keys = sorted({k for m in present_models for k in aggregated_summary[m].get("score_avgs", {}).keys()})
    top_keys = _sort_top_keys(raw_top_keys)

    crit: Dict[str, List[str]] = {}
    for rub in top_keys:
        keys = {k for m in present_models for k in aggregated_summary[m]["criteria_avgs"].get(rub, {}).keys()}
        crit[rub] = _sort_criteria_keys(rub, list(keys))

    # ì¶œë ¥ ë””ë ‰í† ë¦¬
    outdir = Path(OUTPUT_DIR)
    outdir.mkdir(parents=True, exist_ok=True)

    # â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
    # 1. ì „ì²´ í†µí•© ê²°ê³¼ (ëª¨ë“  ë„ë©”ì¸ í‰ê· )
    # â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
    print(f"\n{'='*60}")
    print("ğŸ“Š Generating overall results (all domains combined)...")
    print(f"{'='*60}")
    
    # ì „ì²´ ì‹œê°í™”
    print("  ğŸ¨ Creating visualization...")
    combined_sheet(
        present_models, 
        aggregated_summary, 
        outdir / "overall_comparison.png", 
        DPI, 
        SHOW_PLOT, 
        top_keys, 
        crit,
        title_suffix=" - ALL DOMAINS"
    )
    print(f"  âœ“ Saved: {outdir / 'overall_comparison.png'}")

    # ì „ì²´ Excel
    print("  ğŸ“„ Creating Excel...")
    excel_path = outdir / "evaluation_overall.xlsx"
    export_excel(
        aggregated_summary, 
        excel_path, 
        top_keys, 
        crit
    )
    print(f"  âœ“ Saved: {excel_path}")

    # â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
    # 2. ë„ë©”ì¸ë³„ ê²°ê³¼
    # â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
    print(f"\n{'='*60}")
    print("ğŸ“Š Generating domain-specific results...")
    print(f"{'='*60}")

    for domain in domains:
        if domain not in domain_summaries:
            continue
        
        print(f"\n[{domain.upper()}]")
        domain_data = domain_summaries[domain]
        domain_models = [m for m in present_models if m in domain_data]
        
        if not domain_models:
            print(f"  âš ï¸  No models with data")
            continue

        domain_outdir = outdir / domain
        domain_outdir.mkdir(parents=True, exist_ok=True)

        # ë„ë©”ì¸ ì‹œê°í™”
        print(f"  ğŸ¨ Creating visualization...")
        combined_sheet(
            domain_models, 
            domain_data, 
            domain_outdir / f"{domain}_comparison.png", 
            DPI, 
            SHOW_PLOT, 
            top_keys, 
            crit,
            title_suffix=f" - {domain.upper()}"
        )
        print(f"  âœ“ Saved: {domain_outdir / f'{domain}_comparison.png'}")

        # ë„ë©”ì¸ Excel
        print(f"  ğŸ“„ Creating Excel...")
        excel_path = domain_outdir / f"evaluation_{domain}.xlsx"
        export_excel(
            domain_data, 
            excel_path, 
            top_keys, 
            crit
        )
        print(f"  âœ“ Saved: {excel_path}")

    print(f"\n{'='*60}")
    print("âœ… All processing completed!")
    print(f"ğŸ“ Results saved to: {outdir.absolute()}")
    print(f"{'='*60}\n")

if __name__ == "__main__":
    main()